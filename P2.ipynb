{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "P2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lustea0201/Interpretability/blob/master/P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h4NeSToBbJMn",
        "outputId": "454e09a6-9cb6-4960-9aa7-088cf120c676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import zipfile\n",
        "import io\n",
        "import cv2\n",
        "import glob \n",
        "import numpy as np\n",
        "import random \n",
        "import torch \n",
        "\n",
        "\n",
        "!rm -rf main_dir\n",
        "!rm -rf data.zip\n",
        "\n",
        "method = {\n",
        "    1: \"import locally\", \n",
        "    2: \"from drive\"\n",
        "    }\n",
        "\n",
        "m = 2\n",
        "\n",
        "if (m == 1): \n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  data = zipfile.ZipFile(io.BytesIO(uploaded['data.zip']), 'r')\n",
        "\n",
        "if (m == 2): \n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "\n",
        "  data = zipfile.ZipFile(\"/content/drive/My Drive/Research project/data4/data.zip\", 'r')\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "root_dir = \"main_dir\"\n",
        "data.extractall(root_dir)\n",
        "data.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vAf1Fk2IbguJ",
        "colab": {}
      },
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "transforms = transforms.Compose( [Resize((32, 32), 1), # shouldn't be necessary change later\n",
        "                                  ToTensor(), \n",
        "                                  Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "train_data = ImageFolder(root = os.path.join(root_dir, 'train'), transform = transforms)\n",
        "\n",
        "test_data = ImageFolder(root = os.path.join(root_dir, 'test'), transform = transforms)\n",
        "\n",
        "idx_to_class = {j:i for i,j in train_data.class_to_idx.items()}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cSTJr4lCmYaB",
        "outputId": "692a54e6-b3db-493e-e3c0-eaa5bc688d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "def display_image(image, label): \n",
        "  sample_image = image.numpy().transpose(1,2,0)\n",
        "  mean = np.array([0.5, 0.5, 0.5])\n",
        "  sd = np.array([0.5, 0.5, 0.5])\n",
        "  im = sample_image*sd + mean\n",
        "  plt.imshow(im)\n",
        "  plt.title(\"label : \" + idx_to_class[label])\n",
        "\n",
        "display_image(train_data[-1][0], train_data[-1][1])\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntrQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hoo3HmSLqBbd",
        "colab": {}
      },
      "source": [
        "\n",
        "dataloader = DataLoader(train_data, batch_size = 4, shuffle = True, num_workers = 0)\n",
        "\n",
        "testloader = DataLoader(test_data, batch_size = 4, shuffle = True, num_workers = 0)\n",
        "\n",
        "train_loader = iter(dataloader)\n",
        "x,y = next(train_loader)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q04CfJzYqnVr"
      },
      "source": [
        "https://www.youtube.com/watch?v=fXEdtz8U3Sc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ZHuALqxGmy9",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 2)\n",
        "  \n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 11)\n",
        "    self.pool5 = nn.MaxPool2d(5, 5)\n",
        "    self.pool4 = nn.MaxPool2d(4, 4)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 4)\n",
        "    self.fc1 = nn.Linear(16 * 6 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 2)\n",
        "  \"\"\"\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x = x.view(-1, self.num_flat_features(x)) #or 16 * 5 * 5 ### now 16*6*5\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "  \n",
        "  def num_flat_features(self, x):\n",
        "      size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "      num_features = 1\n",
        "      for s in size:\n",
        "          num_features *= s\n",
        "      return num_features\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxH1U0hnaTzk",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D5-jWYmoaViF",
        "outputId": "4d706f5a-d7bd-4501-9340-cce7b473a92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(4):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "      # get the inputs; data is a list of [inputs, labels]\n",
        "      inputs, labels = data\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      outputs = net(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "          print('[%d, %5d] loss: %.3f' %\n",
        "                (epoch + 1, i + 1, running_loss / 1000))\n",
        "          running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 0.000\n",
            "[1,  2000] loss: 0.000\n",
            "[1,  3000] loss: 0.000\n",
            "[1,  4000] loss: 0.000\n",
            "[1,  5000] loss: 0.000\n",
            "[1,  6000] loss: 0.000\n",
            "[1,  7000] loss: 0.000\n",
            "[1,  8000] loss: 0.000\n",
            "[1,  9000] loss: 0.000\n",
            "[1, 10000] loss: 0.000\n",
            "[1, 11000] loss: 0.000\n",
            "[1, 12000] loss: 0.000\n",
            "[1, 13000] loss: 0.000\n",
            "[1, 14000] loss: 0.000\n",
            "[1, 15000] loss: 0.000\n",
            "[1, 16000] loss: 0.000\n",
            "[1, 17000] loss: 0.000\n",
            "[1, 18000] loss: 0.000\n",
            "[1, 19000] loss: 0.000\n",
            "[1, 20000] loss: 0.000\n",
            "[1, 21000] loss: 0.000\n",
            "[1, 22000] loss: 0.000\n",
            "[1, 23000] loss: 0.000\n",
            "[1, 24000] loss: 0.000\n",
            "[1, 25000] loss: 0.000\n",
            "[1, 26000] loss: 0.000\n",
            "[1, 27000] loss: 0.000\n",
            "[1, 28000] loss: 0.000\n",
            "[1, 29000] loss: 0.000\n",
            "[1, 30000] loss: 0.000\n",
            "[1, 31000] loss: 0.000\n",
            "[1, 32000] loss: 0.000\n",
            "[1, 33000] loss: 0.000\n",
            "[1, 34000] loss: 0.000\n",
            "[1, 35000] loss: 0.000\n",
            "[1, 36000] loss: 0.000\n",
            "[1, 37000] loss: 0.000\n",
            "[1, 38000] loss: 0.000\n",
            "[1, 39000] loss: 0.000\n",
            "[1, 40000] loss: 0.000\n",
            "[1, 41000] loss: 0.000\n",
            "[1, 42000] loss: 0.000\n",
            "[1, 43000] loss: 0.000\n",
            "[1, 44000] loss: 0.000\n",
            "[1, 45000] loss: 0.000\n",
            "[1, 46000] loss: 0.000\n",
            "[1, 47000] loss: 0.000\n",
            "[1, 48000] loss: 0.000\n",
            "[1, 49000] loss: 0.000\n",
            "[1, 50000] loss: 0.003\n",
            "[1, 51000] loss: 0.000\n",
            "[1, 52000] loss: 0.000\n",
            "[2,  1000] loss: 0.000\n",
            "[2,  2000] loss: 0.000\n",
            "[2,  3000] loss: 0.000\n",
            "[2,  4000] loss: 0.000\n",
            "[2,  5000] loss: 0.000\n",
            "[2,  6000] loss: 0.000\n",
            "[2,  7000] loss: 0.000\n",
            "[2,  8000] loss: 0.000\n",
            "[2,  9000] loss: 0.000\n",
            "[2, 10000] loss: 0.000\n",
            "[2, 11000] loss: 0.000\n",
            "[2, 12000] loss: 0.000\n",
            "[2, 13000] loss: 0.000\n",
            "[2, 14000] loss: 0.000\n",
            "[2, 15000] loss: 0.000\n",
            "[2, 16000] loss: 0.000\n",
            "[2, 17000] loss: 0.000\n",
            "[2, 18000] loss: 0.000\n",
            "[2, 19000] loss: 0.000\n",
            "[2, 20000] loss: 0.000\n",
            "[2, 21000] loss: 0.000\n",
            "[2, 22000] loss: 0.000\n",
            "[2, 23000] loss: 0.000\n",
            "[2, 24000] loss: 0.000\n",
            "[2, 25000] loss: 0.000\n",
            "[2, 26000] loss: 0.000\n",
            "[2, 27000] loss: 0.000\n",
            "[2, 28000] loss: 0.000\n",
            "[2, 29000] loss: 0.000\n",
            "[2, 30000] loss: 0.000\n",
            "[2, 31000] loss: 0.000\n",
            "[2, 32000] loss: 0.000\n",
            "[2, 33000] loss: 0.000\n",
            "[2, 34000] loss: 0.000\n",
            "[2, 35000] loss: 0.000\n",
            "[2, 36000] loss: 0.000\n",
            "[2, 37000] loss: 0.000\n",
            "[2, 38000] loss: 0.000\n",
            "[2, 39000] loss: 0.004\n",
            "[2, 40000] loss: 0.000\n",
            "[2, 41000] loss: 0.000\n",
            "[2, 42000] loss: 0.000\n",
            "[2, 43000] loss: 0.000\n",
            "[2, 44000] loss: 0.000\n",
            "[2, 45000] loss: 0.000\n",
            "[2, 46000] loss: 0.000\n",
            "[2, 47000] loss: 0.000\n",
            "[2, 48000] loss: 0.000\n",
            "[2, 49000] loss: 0.000\n",
            "[2, 50000] loss: 0.000\n",
            "[2, 51000] loss: 0.000\n",
            "[2, 52000] loss: 0.000\n",
            "[3,  1000] loss: 0.000\n",
            "[3,  2000] loss: 0.000\n",
            "[3,  3000] loss: 0.000\n",
            "[3,  4000] loss: 0.000\n",
            "[3,  5000] loss: 0.000\n",
            "[3,  6000] loss: 0.000\n",
            "[3,  7000] loss: 0.000\n",
            "[3,  8000] loss: 0.000\n",
            "[3,  9000] loss: 0.000\n",
            "[3, 10000] loss: 0.000\n",
            "[3, 11000] loss: 0.000\n",
            "[3, 12000] loss: 0.000\n",
            "[3, 13000] loss: 0.000\n",
            "[3, 14000] loss: 0.000\n",
            "[3, 15000] loss: 0.000\n",
            "[3, 16000] loss: 0.000\n",
            "[3, 17000] loss: 0.000\n",
            "[3, 18000] loss: 0.000\n",
            "[3, 19000] loss: 0.000\n",
            "[3, 20000] loss: 0.000\n",
            "[3, 21000] loss: 0.000\n",
            "[3, 22000] loss: 0.000\n",
            "[3, 23000] loss: 0.000\n",
            "[3, 24000] loss: 0.000\n",
            "[3, 25000] loss: 0.000\n",
            "[3, 26000] loss: 0.000\n",
            "[3, 27000] loss: 0.000\n",
            "[3, 28000] loss: 0.000\n",
            "[3, 29000] loss: 0.000\n",
            "[3, 30000] loss: 0.000\n",
            "[3, 31000] loss: 0.000\n",
            "[3, 32000] loss: 0.000\n",
            "[3, 33000] loss: 0.000\n",
            "[3, 34000] loss: 0.000\n",
            "[3, 35000] loss: 0.000\n",
            "[3, 36000] loss: 0.000\n",
            "[3, 37000] loss: 0.000\n",
            "[3, 38000] loss: 0.000\n",
            "[3, 39000] loss: 0.000\n",
            "[3, 40000] loss: 0.000\n",
            "[3, 41000] loss: 0.000\n",
            "[3, 42000] loss: 0.000\n",
            "[3, 43000] loss: 0.000\n",
            "[3, 44000] loss: 0.000\n",
            "[3, 45000] loss: 0.000\n",
            "[3, 46000] loss: 0.000\n",
            "[3, 47000] loss: 0.000\n",
            "[3, 48000] loss: 0.000\n",
            "[3, 49000] loss: 0.000\n",
            "[3, 50000] loss: 0.000\n",
            "[3, 51000] loss: 0.000\n",
            "[3, 52000] loss: 0.000\n",
            "[4,  1000] loss: 0.000\n",
            "[4,  2000] loss: 0.000\n",
            "[4,  3000] loss: 0.000\n",
            "[4,  4000] loss: 0.000\n",
            "[4,  5000] loss: 0.000\n",
            "[4,  6000] loss: 0.000\n",
            "[4,  7000] loss: 0.000\n",
            "[4,  8000] loss: 0.000\n",
            "[4,  9000] loss: 0.000\n",
            "[4, 10000] loss: 0.000\n",
            "[4, 11000] loss: 0.000\n",
            "[4, 12000] loss: 0.000\n",
            "[4, 13000] loss: 0.000\n",
            "[4, 14000] loss: 0.000\n",
            "[4, 15000] loss: 0.000\n",
            "[4, 16000] loss: 0.000\n",
            "[4, 17000] loss: 0.000\n",
            "[4, 18000] loss: 0.000\n",
            "[4, 19000] loss: 0.000\n",
            "[4, 20000] loss: 0.000\n",
            "[4, 21000] loss: 0.000\n",
            "[4, 22000] loss: 0.000\n",
            "[4, 23000] loss: 0.000\n",
            "[4, 24000] loss: 0.000\n",
            "[4, 25000] loss: 0.000\n",
            "[4, 26000] loss: 0.000\n",
            "[4, 27000] loss: 0.000\n",
            "[4, 28000] loss: 0.000\n",
            "[4, 29000] loss: 0.000\n",
            "[4, 30000] loss: 0.000\n",
            "[4, 31000] loss: 0.000\n",
            "[4, 32000] loss: 0.000\n",
            "[4, 33000] loss: 0.000\n",
            "[4, 34000] loss: 0.000\n",
            "[4, 35000] loss: 0.000\n",
            "[4, 36000] loss: 0.000\n",
            "[4, 37000] loss: 0.000\n",
            "[4, 38000] loss: 0.000\n",
            "[4, 39000] loss: 0.000\n",
            "[4, 40000] loss: 0.000\n",
            "[4, 41000] loss: 0.000\n",
            "[4, 42000] loss: 0.000\n",
            "[4, 43000] loss: 0.000\n",
            "[4, 44000] loss: 0.000\n",
            "[4, 45000] loss: 0.000\n",
            "[4, 46000] loss: 0.000\n",
            "[4, 47000] loss: 0.000\n",
            "[4, 48000] loss: 0.000\n",
            "[4, 49000] loss: 0.000\n",
            "[4, 50000] loss: 0.000\n",
            "[4, 51000] loss: 0.000\n",
            "[4, 52000] loss: 0.000\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CSoLjUOliRxh",
        "outputId": "06a16e9a-d7b2-46bd-b790-ad16a2310b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "wrong_im = []\n",
        "wrong_label = []\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        k = 0\n",
        "        for truth in (predicted == labels): \n",
        "          if not truth: \n",
        "            wrong_im.append(images[k])\n",
        "            wrong_label.append(int(labels[k]))\n",
        "          k += 1\n",
        "        \n",
        "        \n",
        "\n",
        "print(\"Accuracy of the network on the \" + str(total) + ' test images: %d %%' % (\n",
        "    100 * correct / total))\n",
        "\n",
        "print(\"Number of mistakes : \" +str(total-correct))\n",
        "index = 0"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 90000 test images: 99 %\n",
            "Number of mistakes : 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_2qbFlsY0EY3",
        "outputId": "ebb570c8-81d1-4ffe-b8bc-d8efac1df364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "if index in range(len(wrong_im)):\n",
        "  display_image(wrong_im[index], wrong_label[index])\n",
        "  index += 1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4bCbJeGwmyXhsJsl4bCbJeGwmyXx/6Rco3mpQIxvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}