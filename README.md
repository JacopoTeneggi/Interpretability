# Interpretability
This repo contains my work for a Biomedical Engineering Undergraduate Research course at Johns Hopkins University.
## Aim 
Study and compare various methods to produce saliency maps on a synthetic dataset. These methods are ueful in understanding what parts of an image are important for predictions made by a neural network. 
Present a modified version of the SHAP method, which functions in a recursive and hierarchical manner to produce exact Shapley coefficients. 

## References 
### Pubications 
#### General
- Adebayo et al., Sanity Checks for Saliency Maps, [online], https://arxiv.org/abs/1810.03292
- Lundberg SM et al. (2018), Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. Nature Biomedical Engineering 2(10):749â€“760, https://www.nature.com/articles/s41551-018-0304-0
#### Gradient-based methods 
- Selvaraju et al., Grad-cam: Visual explanations from deep networks via gradient-based localization, [online],  https://arxiv.org/abs/1610.02391v3.
- Chattopadhyay et al., Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks, [online], https://arxiv.org/abs/1710.11063
#### Shapley-based methods 
- Chen et al., L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data, [online], https://arxiv.org/abs/1808.02610
- 

