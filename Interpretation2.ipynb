{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interpretation2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGvhQzcO9D5UPJ9ykWnrdH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lustea0201/Interpretability/blob/master/Interpretation2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJWLB_GHkY_1",
        "colab_type": "text"
      },
      "source": [
        "# Interpreting the outputs of the model on the second dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy5jZvelpXpl",
        "colab_type": "code",
        "outputId": "44852137-706b-4853-9872-b83e02a43781",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        }
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import torch \n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!pip install flashtorch\n",
        "from flashtorch.saliency import Backprop\n",
        "\n",
        "!pip install shap \n",
        "import shap\n",
        "\n",
        "!pip install pytorch-gradcam\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "from gradcam.utils import visualize_cam\n",
        "from gradcam import GradCAM, GradCAMpp\n",
        "\n",
        "!wget https://raw.githubusercontent.com/yiskw713/SmoothGradCAMplusplus/master/cam.py -P local_modules -nc\n",
        "!wget https://raw.githubusercontent.com/yiskw713/SmoothGradCAMplusplus/master/utils/visualize.py -P local_modules -nc\n",
        "import sys\n",
        "sys.path.append('local_modules')\n",
        "from PIL import Image\n",
        "from local_modules.visualize import visualize\n",
        "import local_modules.cam as smooth\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lustea0201/Interpretability/master/HierarchicalShapley.py -P local_modules -nc\n",
        "import local_modules.HierarchicalShapley as HS\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lustea0201/Interpretability/master/utils.py -P local_modules -nc\n",
        "import local_modules.utils as utils\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lustea0201/Interpretability/master/interpret11.py -P local_modules -nc\n",
        "import local_modules.interpret11 as intp\n",
        "\n",
        "torch.manual_seed(0)\n",
        "dtype = torch.float\n",
        "\n",
        "FIGSIZE = (5,4)\n",
        "\n",
        "net = utils.Net() # Model instantiation\n",
        "# Loading the trained dictionnary state\n",
        "net.load_state_dict(torch.load('drive/My Drive/Interpretability/model2.pth')) \n",
        "\n",
        "data = zipfile.ZipFile(\"/content/drive/My Drive/Interpretability/5000/data2/data.zip\", 'r')\n",
        "\n",
        "root_dir = \"main_dir\"\n",
        "data.extractall(root_dir)\n",
        "data.close()\n",
        "\n",
        "Batch_Size = 64\n",
        "#transf_temp =  transforms.Compose( [ToTensor()])\n",
        "#train_data_temp = ImageFolder(root = os.path.join(root_dir, 'train'), transform = transf_temp)\n",
        "#dataloader_temp = DataLoader(train_data_temp, batch_size = Batch_Size, shuffle = True, num_workers = 0)\n",
        "#MEAN, STD = utils.datasetMeanStd(dataloader_temp)\n",
        "MEAN, STD = np.array([0.5, 0.5, 0.5]), np.array([0.5, 0.5, 0.5])\n",
        "\n",
        "transf = transforms.Compose( [ToTensor(), Normalize(mean=MEAN, std=STD)])\n",
        "\n",
        "train_data = ImageFolder(root = os.path.join(root_dir, 'train'), transform = transf)\n",
        "dataloader = DataLoader(train_data, batch_size = Batch_Size, shuffle = True, num_workers = 0)\n",
        "train_loader = iter(dataloader)\n",
        "\n",
        "\n",
        "ImF = ImageFolder(root = \"/content/drive/My Drive/Interpretability/img4saliencymap/data2\", transform = transf)\n",
        "batch_Size = 3\n",
        "exloader = DataLoader(ImF, batch_size = batch_Size, shuffle = False, num_workers = 0)\n",
        "\n",
        "exIter = iter(exloader)\n",
        "images, labels = next(exIter)\n",
        "\n",
        "# For guided backprop\n",
        "backprop = Backprop(net)\n",
        "\n",
        "# For SHAP\n",
        "X,Y = next(train_loader)\n",
        "bg_choice = \"average\" # Default: entire training set, \"average\", \"white\", \"black\" \n",
        "# Median would result in white \n",
        "background = X\n",
        "if (bg_choice == \"black\"): \n",
        "  background = -torch.ones(X.shape)\n",
        "elif (bg_choice == \"white\"): \n",
        "  background = torch.ones(X.shape)\n",
        "elif (bg_choice == \"average\"): \n",
        "  avg = torch.mean(X, dim = 0)\n",
        "  display_image(avg, 0)\n",
        "e = shap.GradientExplainer(net, background)\n",
        "\n",
        "# For GradCAM\n",
        "gradcam_conv = GradCAM(net, net.conv2)\n",
        "gradcam_pp_conv = GradCAMpp(net, net.conv2)\n",
        "gradcam_pool = GradCAM(net, net.pool2)\n",
        "gradcam_pp_pool = GradCAMpp(net, net.pool2)\n",
        "\n",
        "# For SmoothCAM\n",
        "target_layer = net.conv2\n",
        "wrapped_G = smooth.GradCAM(net, target_layer)\n",
        "wrapped_P = smooth.GradCAMpp(net, target_layer)\n",
        "wrapped_S = smooth.SmoothGradCAMpp(net, target_layer, n_samples=25, stdev_spread=0.15)\n",
        "wrapped = [wrapped_G, wrapped_P, wrapped_S]\n",
        "\n",
        "# For Hierarchical \n",
        "white_bg = torch.ones(images.shape[1:])\n",
        "h = HS.HierarchicalShap(net, background = white_bg)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Requirement already satisfied: flashtorch in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from flashtorch) (1.18.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flashtorch) (0.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from flashtorch) (1.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from flashtorch) (1.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from flashtorch) (7.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from flashtorch) (3.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->flashtorch) (1.12.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources->flashtorch) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources->flashtorch) (3.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flashtorch) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flashtorch) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flashtorch) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flashtorch) (2.8.1)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.6/dist-packages (0.35.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.18.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.0.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.38.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->shap) (1.12.0)\n",
            "Requirement already satisfied: pytorch-gradcam in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from pytorch-gradcam) (4.1.2.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-gradcam) (1.18.2)\n",
            "File ‘local_modules/cam.py’ already there; not retrieving.\n",
            "\n",
            "File ‘local_modules/visualize.py’ already there; not retrieving.\n",
            "\n",
            "File ‘local_modules/HierarchicalShapley.py’ already there; not retrieving.\n",
            "\n",
            "File ‘local_modules/utils.py’ already there; not retrieving.\n",
            "\n",
            "File ‘local_modules/interpret11.py’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-5cd684caf96a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbg_choice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"average\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m   \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m   \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'display_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj-YQEPzpu37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h.saliency_map(images[1], labels[1], tolerance = [2,3,4,5,6], only_one_run = False, max_depth = 30, debug = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6DZoWtfwTp6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# EXAMPLE 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrXPeA60p9Xb",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "image0 = images[0]\n",
        "label0 = labels[0]\n",
        "\n",
        "input0 = image0.view(-1, 3, 100, 120) # This shape is necessary for the network \n",
        "output0 = net(input0)\n",
        "_, predicted0 = torch.max(output0.data, 1)\n",
        "\n",
        "img0 = utils.input2image(image0, MEAN, STD)\n",
        "utils.display_image(img0, label0.numpy(), int(predicted0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34jSkq_pqmSy",
        "colab_type": "text"
      },
      "source": [
        "### Guided backpropagation explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvwjEy9Etipy",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "input0.requires_grad = True # Necessary to compute the gradient\n",
        "gradients0 = backprop.calculate_gradients(input0, label0, guided = True).detach().numpy()\n",
        "intp.display_gradients(gradients0, FIGSIZE).suptitle(\"Guided backpropagation across the different channels for an image with label 0\", size=\"xx-large\");\n",
        "backprop.visualize(input0, label0, guided=True, figsize = (3*FIGSIZE[0], FIGSIZE[1]), cmap=\"bwr\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCH4yuMSdjCd",
        "colab_type": "text"
      },
      "source": [
        "### GradCAM explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L14e_TwWdjOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intp.gradcam_exp(gradcam_conv, gradcam_pp_conv, input0, img0,  \"Second convolutional layer\", FIGSIZE)\n",
        "intp.gradcam_exp(gradcam_pool, gradcam_pp_pool, input0, img0,  \"Second pooling layer\", FIGSIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6lDStxxdyxE",
        "colab_type": "text"
      },
      "source": [
        "### SmoothCAM explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivPV8F_xdyLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intp.smooth_exp(input0, image0, wrapped)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "525Zk4cvdaqz",
        "colab_type": "text"
      },
      "source": [
        "### Shapley explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOCXfrZLdKug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intp.shap_exp(e, input0.detach(), img0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWbPaiUrBc2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e2 = shap.DeepExplainer(net, background)\n",
        "\n",
        "\n",
        "shap_values = e2.shap_values(input0.detach())\n",
        "shapley_values = [np.swapaxes(np.swapaxes(s, 2, 3), 1, -1) for s in shap_values]\n",
        "\n",
        "image00 = img0[np.newaxis, :]\n",
        "\n",
        "shap.image_plot(shapley_values, image00,  show=False)\n",
        "plt.suptitle(\"All channels together\");\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6epZU5m3eZ-s",
        "colab_type": "text"
      },
      "source": [
        "### Hierarchical Shapley explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gLgXcSUeaLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h.saliency_map(image0, label0, tolerance  = [5,5.5,6,6.5,7])\n",
        "h.saliency_map(image0, label0, tolerance = [6], only_one_run = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a16oKk-gKl7O",
        "colab_type": "text"
      },
      "source": [
        "# EXAMPLE 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6ghRSmzeOok",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "image1 = images[1]\n",
        "label1 = labels[1]\n",
        "\n",
        "input1 = image1.view(-1, 3, 100, 120)\n",
        "output1 = net(input1)\n",
        "_, predicted1 = torch.max(output1.data, 1)\n",
        "\n",
        "img1 = utils.input2image(image1, MEAN, STD)\n",
        "utils.display_image(img1, label1.numpy(), int(predicted1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F2SksFyfNyo",
        "colab_type": "text"
      },
      "source": [
        "### Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZc1AkJCe7hJ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "input1.requires_grad = True # Necessary to compute the gradient\n",
        "gradients1 = backprop.calculate_gradients(input1, label1, guided = True).detach().numpy()\n",
        "intp.display_gradients(gradients1, FIGSIZE).suptitle(\"Guided backpropagation across the different channels for an image with label 0\", size=\"xx-large\");\n",
        "backprop.visualize(input1, label1, guided=True, figsize = (3*FIGSIZE[0], FIGSIZE[1]), cmap=\"bwr\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYzNEkgPfTVe",
        "colab_type": "text"
      },
      "source": [
        "### GradCAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3KonhoofTfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intp.gradcam_exp(gradcam_conv, gradcam_pp_conv, input1, img1,  \"Second convolutional layer\", FIGSIZE)\n",
        "intp.gradcam_exp(gradcam_pool, gradcam_pp_pool, input1, img1,  \"Second pooling layer\", FIGSIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UtIzMN0fs1e",
        "colab_type": "text"
      },
      "source": [
        "### SmoothCAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6KJNxBCfs71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intp.smooth_exp(input1, image1, wrapped)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjuiiMADfRvL",
        "colab_type": "text"
      },
      "source": [
        "### Shapley explanation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzrdl9YIfS-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "intp.shap_exp(e, input1.detach(), img1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSW8zYH8ETt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shap_values = e2.shap_values(input1.detach())\n",
        "shapley_values = [np.swapaxes(np.swapaxes(s, 2, 3), 1, -1) for s in shap_values]\n",
        "\n",
        "image11 = img1[np.newaxis, :]\n",
        "\n",
        "shap.image_plot(shapley_values, image11,  show=False)\n",
        "plt.suptitle(\"All channels together\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcA5fUFSOGpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(image11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UkaPvmWftF9",
        "colab_type": "text"
      },
      "source": [
        "### Hierarchical Shapley"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDQ_FdYlftNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "a = 6.0\n",
        "f = np.linspace(0.7, 1.3, 30)\n",
        "l = (f*a).tolist()\n",
        "h.saliency_map(image1, label1, tolerance = l)\n",
        "h.saliency_map(image1, label1, tolerance = [6], only_one_run = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LONuoeMizdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = ImageFolder(root = os.path.join(root_dir, 'train'), transform = transf)\n",
        "dataloader = DataLoader(train_data, batch_size = Batch_Size, shuffle = True, num_workers = 0)\n",
        "train_loader = iter(dataloader)\n",
        "\n",
        "images, labels = next(train_loader)\n",
        "i = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63-8OVM-jR-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(20):\n",
        "  image = images[i]\n",
        "  label = labels[i]\n",
        "\n",
        "  input_ = image.view(-1, 3, 100, 120) # This shape is necessary for the network \n",
        "  output = net(input_)\n",
        "  _, predicted = torch.max(output.data, 1)\n",
        "  a = output.data[0][1].detach().numpy()\n",
        "  img = utils.input2image(image, MEAN, STD)\n",
        "  utils.display_image(img, a, int(predicted))\n",
        "  \n",
        "\n",
        "  if (a > 0):\n",
        "    h.saliency_map(image, label, tolerance = l, only_one_run = False)\n",
        "  i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}